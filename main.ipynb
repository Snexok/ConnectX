{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bitbaseconda355ca08b8cf849dca27281b283aed1dd",
   "display_name": "Python 3.7.5 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "import torch\n",
    "from torch import tensor, save, load, optim, cat, zeros\n",
    "import torch.nn.functional as F\n",
    "from metrics import mean_reward_with_enemy as mean_reward\n",
    "from memory import Memory\n",
    "from model import Model\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from submit import write_agent\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    from submission import model\n",
    "    save(model.state_dict(), 'model')\n",
    "    model = Model()\n",
    "    model.load_state_dict(load('model'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(policy_model, target_model, memory, optimizer, BATCH_SIZE):\n",
    "    GAMMA = 0.99\n",
    "    if not (memory==500):\n",
    "        return False\n",
    "    _obs_batch, act_batch, obs_batch, reward_batch = zip(*memory.sample(BATCH_SIZE))\n",
    "\n",
    "    _obs_batch = tensor(tuple(map(lambda s: tuple(s), _obs_batch)), device=device)\n",
    "    act_batch = tensor(act_batch, device=device).unsqueeze(1)\n",
    "    non_final_obs = tensor([tuple(s) for s in obs_batch if s is not None], device=device)\n",
    "    non_final_mask = tensor(tuple(map(lambda s: s is not None, obs_batch)), device=device, dtype=torch.bool)\n",
    "    reward_batch = tensor(reward_batch, device=device).unsqueeze(1)\n",
    "\n",
    "    policy_action_values = policy_model(_obs_batch).gather(1, act_batch)\n",
    "    target_action_values = zeros(BATCH_SIZE, device=device)\n",
    "    target_action_values[non_final_mask] = target_model(non_final_obs).max(-1)[0].detach()\n",
    "    expected_state_action_values = (target_action_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.smooth_l1_loss(policy_action_values, target_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_model.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0\n25\n50\n75\n100\n125\n150\n175\n200\n225\n250\n275\n300\n325\n350\n375\n400\n425\n450\n475\n500\n525\n550\n575\n600\n625\n650\n675\n700\n725\n750\n775\n800\n825\n850\n875\n900\n925\n950\n975\n"
    }
   ],
   "source": [
    "def train(model,\n",
    "          episods=1000,\n",
    "          memory_size = 1000\n",
    "          ):\n",
    "    TARGET_UPDATE = 25\n",
    "    env = make(\"connectx\")\n",
    "    trainer = env.train([None, \"negamax\"])\n",
    "    p_model = model\n",
    "    t_model = Model()\n",
    "    t_model.load_state_dict(p_model.state_dict())\n",
    "    memory = Memory(memory_size)\n",
    "    p_model.to(device)\n",
    "    t_model.to(device)\n",
    "    t_model.train()\n",
    "    optimizer = optim.RMSprop(p_model.parameters())\n",
    "    for e in range(episods):\n",
    "        obs = trainer.reset()\n",
    "        obs = tensor(obs.board, dtype=torch.float)\n",
    "        while not env.done:\n",
    "            act = 0\n",
    "            _obs = obs\n",
    "            if(memory==memory_size):\n",
    "                pred = model(obs.to(device))\n",
    "                act = int(pred.data.max(-1)[1])\n",
    "            else:\n",
    "                rnd_pred = tensor([[random.randrange(7)]], dtype=torch.int)\n",
    "                act = int(rnd_pred)\n",
    "                # act = int(random.choice(np.argwhere(np.array(obs.board)[:7]==0))[0])\n",
    "            # previous_observation, action, observation, reward, done, info\n",
    "            obs, reward, done, _ = trainer.step(act)\n",
    "            obs = tensor(obs.board, dtype=torch.float)\n",
    "            if done == 1 or reward is None:\n",
    "                obs = None\n",
    "            if(reward is None):\n",
    "                reward = -1\n",
    "            state = [_obs, act, obs, reward]\n",
    "            memory.memorize(state)\n",
    "            optimize_model(p_model, t_model, memory, optimizer, 64)\n",
    "        if e % TARGET_UPDATE == 0:\n",
    "            print(e)\n",
    "            t_model.load_state_dict(p_model.state_dict())\n",
    "\n",
    "    return p_model\n",
    "\n",
    "            # if(done):\n",
    "            #     reward += 10\n",
    "        # env.render(mode='ipython')\n",
    "model = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_agent(model.paramsToList())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission import my_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "My Agent vs Random Agent: 0.86\nMy Agent vs Random Agent: 0.79\nMy Agent vs Random Agent: 0.82\nMy Agent vs Random Agent: 0.76\nMy Agent vs Random Agent: 0.855\n"
    }
   ],
   "source": [
    "model = Model()\n",
    "def my_agent(observation, configuration):\n",
    "    pred = model(torch.tensor(observation.board, dtype=torch.float))\n",
    "    pred = int(pred.data.max(-1)[1])\n",
    "    act =  pred if observation.board[pred] == 0 else random.choice(np.argwhere(np.array(observation.board)[:7]==0))[0]\n",
    "    return int(act)\n",
    "for i in range(5):\n",
    "    mean_reward(my_agent, 'random', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "My Agent vs Random Agent: 0.0\nMy Agent vs Random Agent: 0.0\nMy Agent vs Random Agent: 0.0\nMy Agent vs Random Agent: 0.0\nMy Agent vs Random Agent: 0.0\n"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    mean_reward('negamax', 'negamax', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "My Agent vs Random Agent: 0.0\nMy Agent vs Random Agent: 0.0\nMy Agent vs Random Agent: 0.0\nMy Agent vs Random Agent: 0.0\nMy Agent vs Random Agent: 0.0\n"
    }
   ],
   "source": [
    "model_2 = Model()\n",
    "def my_agent_2(observation, configuration):\n",
    "    pred = model_2(torch.tensor(observation.board, dtype=torch.float))\n",
    "    pred = int(pred.data.max(-1)[1])\n",
    "    act =  pred if observation.board[pred] == 0 else random.choice(np.argwhere(np.array(observation.board)[:7]==0))[0]\n",
    "    return int(act)\n",
    "for i in range(5):\n",
    "    mean_reward(my_agent, my_agent_2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 2 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 2 | 0 | 0 | 0 | 1 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 2 | 0 | 0 | 0 | 1 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 2 | 0 | 0 | 0 | 1 | 1 | 0 |\n+---+---+---+---+---+---+---+\n\n"
    }
   ],
   "source": [
    "env = make(\"connectx\")\n",
    "env.run([my_agent, my_agent_2])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}